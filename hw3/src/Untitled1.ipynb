{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "img_number = tag.iloc[n,0]\n",
    "img_tag = tag.iloc[n,1]\n",
    "\n",
    "print('Image number: {}'.format(img_number))\n",
    "print('Image tag: {}'.format(img_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img_tags(image, tag):\n",
    "    \"\"\"Show image with tags\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.title(tag)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "plt.figure()\n",
    "show_img_tags(io.imread(os.path.join('../extra_data/images/', str(img_number)+'.jpg')),\n",
    "             img_tag)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anime_tag_dataset = anime_tag_Dataset(csv_file='../extra_data/tags.csv',\n",
    "                                     root_dir='../extra_data/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = pd.read_csv('../extra_data/tags.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_size = 2\n",
    "embedding_dim = 30\n",
    "raw_text = ['orange hair', 'white hair', 'aqua hair', 'gray hair',\n",
    "'green hair', 'red hair', 'purple hair', 'pink hair',\n",
    "'blue hair', 'black hair', 'brown hair', 'blonde hair',\n",
    "'gray eyes', 'black eyes', 'orange eyes',\n",
    "'pink eyes', 'yellow eyes', 'aqua eyes', 'purple eyes',\n",
    "'green eyes', 'brown eyes', 'red eyes', 'blue eyes']\n",
    "\n",
    "vocab = set()\n",
    "for i in raw_text:\n",
    "    vocab.update(i.split())\n",
    "word_to_idx = {word: float(i) for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class anime_tag_Dataset(Dataset):\n",
    "    \"\"\"Anime tag dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with tags.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.tag_csv = pd.read_csv(csv_file, header=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tag_csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                str(self.tag_csv.iloc[idx, 0])+'.jpg')\n",
    "        image = io.imread(img_name)\n",
    "        tags = self.tag_csv.iloc[idx, 1]\n",
    "        sample = {'image': image, 'tags': tags}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Nornalize images\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, tags = sample['image'], sample['tags']\n",
    "        image = transforms.functional.normalize(image,\n",
    "                                mean=(0.5, 0.5, 0.5),\n",
    "                                std=(0.5, 0.5, 0.5))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'tags': torch.from_numpy(tags)}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, tags = sample['image'], sample['tags']\n",
    "        tags = [word_to_idx[w] for w in tags.split()]\n",
    "        tags = np.array(tags)\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'tags': torch.from_numpy(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = anime_tag_Dataset(csv_file='../extra_data/tags.csv',\n",
    "                                     root_dir='../extra_data/images/',\n",
    "                                    transform=transforms.Compose([\n",
    "                                        ToTensor(),\n",
    "                                    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(transformed_dataset, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for i_batch, sample_batched in enumerate(dataloader):\n",
    "#    print(i_batch, sample_batched['image'], sample_batched['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=100, label_size=4, num_classes=3*64*64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "                    nn.Linear(input_size+label_size, 256),\n",
    "                    nn.LeakyReLU(0.2),\n",
    "                    nn.Linear(256,256),\n",
    "                    nn.LeakyReLU(0.2),\n",
    "                    nn.Linear(256, num_classes),\n",
    "                    nn.Tanh()\n",
    "        )\n",
    "    def forward(self, image, condition):\n",
    "        x, y = image.view(image.size(0), -1), condition.view(condition.size(0), -1).float()\n",
    "        v = torch.cat((x,y), 1)\n",
    "        output = self.layer(v)\n",
    "        output = output.view(x.size(0), 3, 64, 64)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple Discriminator w/ MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=64*64*3, label_size=4, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_size+4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        v = torch.cat([x,y.type_as(x)], 1) # v: [input, label] concatenated vector\n",
    "        y_ = self.layer1(v)\n",
    "        y_ = self.layer2(y_)\n",
    "        y_ = self.layer3(y_)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'ConditionalGAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator()\n",
    "G = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "D_opt = torch.optim.Adam(D.parameters())\n",
    "G_opt = torch.optim.Adam(G.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch  = 100 # need more than 200 epochs for training generator\n",
    "step = 0\n",
    "n_critic = 5 # for training more k steps about Discriminator\n",
    "n_noise = 100\n",
    "\n",
    "D_labels = torch.ones(BATCH_SIZE)#.cuda() # Discriminator Label to real\n",
    "D_fakes = torch.zeros(BATCH_SIZE)#.cuda() # Discriminator Label to fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  253  255  253  ...   250  245  246\n",
      "  254  253  255  ...   252  248  247\n",
      "  255  255  249  ...   250  247  245\n",
      "      ...         ⋱        ...      \n",
      "   34   28   29  ...   247  249  247\n",
      "   30   28   31  ...   248  248  248\n",
      "   31   30   37  ...   248  247  249\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  249  254  251  ...   249  243  246\n",
      "  250  249  254  ...   251  246  247\n",
      "  253  253  247  ...   249  245  245\n",
      "      ...         ⋱        ...      \n",
      "   38   32   31  ...   247  248  243\n",
      "   34   32   35  ...   248  247  244\n",
      "   35   34   41  ...   248  246  245\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  248  253  252  ...   247  244  248\n",
      "  249  250  255  ...   249  247  249\n",
      "  254  254  250  ...   247  246  247\n",
      "      ...         ⋱        ...      \n",
      "   50   44   44  ...   247  246  242\n",
      "   46   44   47  ...   248  245  243\n",
      "   47   46   53  ...   248  244  244\n",
      "[torch.FloatTensor of size 1x3x64x64]\n",
      "\n",
      "Variable containing:\n",
      " 5  8  5  3\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.FloatTensor' object has no attribute 'requires_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4d787086f33c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mD_x_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \"\"\"\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         return F.binary_cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m    430\u001b[0m                                       size_average=self.size_average)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m_assert_no_grad\u001b[0;34m(variable)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m\"nn criterions don't compute the gradient w.r.t. targets - please \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"mark these variables as volatile or not requiring gradients\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.FloatTensor' object has no attribute 'requires_grad'"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    for idx, sample in enumerate(dataloader):\n",
    "        step += 1 \n",
    "        # Training Discriminator\n",
    "        x = Variable(sample['image'].float(), requires_grad = False)\n",
    "        y = Variable(sample['tags'].float(), requires_grad = False)\n",
    "        print(x)\n",
    "        print(y)\n",
    "        x_outputs = D(x, y)\n",
    "        D_x_loss = criterion(x_outputs, D_labels)\n",
    "\n",
    "        z = to_cuda(torch.randn(BATCH_SIZE, n_noise))\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        D_z_loss = criterion(z_outputs, D_fakes)\n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "        \n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_opt.step()\n",
    "        \n",
    "        if step % n_critic == 0:\n",
    "            # Training Generator\n",
    "            z = torch.randn(BATCH_SIZE, n_noise)#.cuda()\n",
    "            z_outputs = D(G(z, y), y)\n",
    "            G_loss = criterion(z_outputs, D_labels)\n",
    "\n",
    "            G.zero_grad()\n",
    "            G_loss.backward()\n",
    "            G_opt.step()\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print('Epoch: {}/{}, Step: {}, D Loss: {}, G Loss: {}'.format(epoch, max_epoch, step, D_loss.data[0], G_loss.data[0]))\n",
    "            \n",
    "        if epoch % 5 == 0:\n",
    "            G.eval()\n",
    "            img = get_sample_image(G)\n",
    "            scipy.misc.imsave('sample/{}_epoch_{}_type1.jpg'.format(MODEL_NAME, epoch), img)\n",
    "            G.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    for idx, (images, labels) in enumerate(dataloader):\n",
    "        print(idx, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
